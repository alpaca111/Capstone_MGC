{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display   #for loading and visualizing audio files\n",
    "import IPython.display as ipd   #to play audio\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "import time \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting audio time series | ATS\n",
    "\n",
    "- Extracting via librosa\n",
    "- Storing as numpy arrays in corresponding harddrive\n",
    "- Collecting track_id and associated file name - identifying corrupt tracks with error list\n",
    "- Storing this in a dictionary file with pickle\n",
    "- Limit iteration procedure to separate in collection chunks (1st 50 folders, 2nd 50 etc) - modify for loop to allow flexible range adjustment\n",
    "- Set number of folders upper limit : 100 folders ~ 16,000 tracks\n",
    "\n",
    "CARE IN RUNNING below TWICE as will overwrite already saved files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get all filepaths for a given folder and store the audio time series y, sr\n",
    "fp_main = '/Volumes/Extreme SSD/CAPSTONE_DATA/fma_medium/'\n",
    "folders = os.listdir(fp_main)\n",
    "\n",
    "numbers = ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "dictionary = {'track_id': [], 'folder': []} #empty dictionary to fill\n",
    "error_list = []  \n",
    "\n",
    "for i in range(folders.index('051'), folders.index('100') + 1):  #nice way for flexible range adjustment\n",
    "    \n",
    "    if any(x in folders[i] for x in numbers):  #prevent picking up hidden files e.g 'README.txt' or 'checksums'\n",
    "        print(\"folder :\",folders[i])\n",
    "        fp_main_new = fp_main + folders[i] + '/'  #not forget the '/' at the end\n",
    "        audio_clips = os.listdir(fp_main_new)\n",
    "        for track_id in audio_clips:\n",
    "            if '_' in track_id:\n",
    "                pass\n",
    "            else:\n",
    "                fp_new = fp_main_new + track_id    \n",
    "                try:     #need try and except - some clips are corrupted e.g '001486.mp3'\n",
    "                    \n",
    "                    y,sr = librosa.load(fp_new, duration = 30) #clips are 30secs #if doesn't collect won't save\n",
    "                    np.save('/Volumes/Extreme SSD/CAPSTONE_DATA/saved/ATS_{}_{}/{}.npy'.format(51, 100, track_id.split('.')[0]), y)\n",
    "                    \n",
    "                    dictionary['track_id'].append(track_id.split('.')[0])\n",
    "                    dictionary['folder'].append(folders[i])\n",
    "                except:\n",
    "                    error_list.append((folders[i],track_id.split('.')[0]))\n",
    "        \n",
    "                    \n",
    "        a_dict = open('/Volumes/Extreme SSD/CAPSTONE_DATA/saved/dictionaries/dict_ATS_{}_{}.pkl'.format(51,100), \"wb\")\n",
    "        pickle.dump(dictionary, a_dict)\n",
    "        a_dict.close()\n",
    "        \n",
    "        a_txt = open('/Volumes/Extreme SSD/CAPSTONE_DATA/saved/dictionaries/errors_{}_{}.txt'.format(51,100), 'wb')\n",
    "        pickle.dump(error_list, a_txt)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Displaying error txt file, dictionary with ATS track_id's and ATS extracted numpy array for 1 track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors found in folders 0 - 50: [('001', '001486'), ('005', '005574')]\n",
      "Errors found in folders 51 - 100: [('065', '065753'), ('080', '080391'), ('098', '098558'), ('098', '098559'), ('098', '098560'), ('098', '098571'), ('099', '099134')]\n"
     ]
    }
   ],
   "source": [
    "a_txt = open('/Volumes/Extreme SSD/CAPSTONE_DATA/saved/dictionaries/errors_0_50.txt', \"rb\")\n",
    "a_txt_51_100 = open('/Volumes/Extreme SSD/CAPSTONE_DATA/saved/dictionaries/errors_51_100.txt', \"rb\")\n",
    "errors_0_50 = pickle.load(a_txt)\n",
    "errors_51_100 = pickle.load(a_txt_51_100)\n",
    "print(\"Errors found in folders 0 - 50:\", errors_0_50)\n",
    "print(\"Errors found in folders 51 - 100:\",errors_51_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading dictionaries for 0 - 50 & 51 - 100 containing track audio files and corresponding folder of each track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a_file = open(\"/Volumes/Extreme SSD/CAPSTONE_DATA/saved/dictionaries/dict_ATS_0_50.pkl\", \"rb\")\n",
    "dict_ATS_0_50 = pickle.load(a_file)\n",
    "\n",
    "a_file_51_100 = open(\"/Volumes/Extreme SSD/CAPSTONE_DATA/saved/dictionaries/dict_ATS_51_100.pkl\", \"rb\")\n",
    "dict_ATS_51_100 = pickle.load(a_file_51_100)\n",
    "\n",
    "#print(dict_ATS_0_50)\n",
    "#print(dict_ATS_51_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that array shape of audio time series caps at 661500 as this is limit of track duration (30 seconds) set above in extraction but an audio file might be slightly shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATS array shape :  (661248,)\n",
      "ATS array shape :  (661500,)\n",
      "ATS array shape :  (661500,)\n",
      "ATS array shape :  (661248,)\n",
      "ATS array shape :  (661500,)\n",
      "ATS array shape :  (661248,)\n",
      "ATS array shape :  (661248,)\n",
      "ATS array shape :  (661248,)\n",
      "ATS array shape :  (661248,)\n",
      "ATS array shape :  (661248,)\n"
     ]
    }
   ],
   "source": [
    "for i in dict_ATS_0_50['track_id'][0:10]:\n",
    "    a_array = open('/Volumes/Extreme SSD/CAPSTONE_DATA/saved/ATS_0_50/{}.npy'.format(i), \"rb\")\n",
    "    ATS_050993 = np.load(a_array)\n",
    "    print(\"ATS array shape : \", ATS_050993.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking number of tracks collected so far. Also checking numb of ATS arrays collected so far matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of tracks in folders 000 - 050 : 7949\n",
      "No. of tracks in folders 051 - 100 : 8073\n"
     ]
    }
   ],
   "source": [
    "df_files_0_50 = pd.DataFrame(dict_ATS_0_50)\n",
    "df_files_0_50.head()\n",
    "df_files_51_100 = pd.DataFrame(dict_ATS_51_100)\n",
    "print(\"No. of tracks in folders 000 - 050 :\", len(df_files_0_50.track_id))\n",
    "print(\"No. of tracks in folders 051 - 100 :\", len(df_files_51_100.track_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of arrays in folders 000 - 050 : 7949\n",
      "No. of arrays in folders 051 - 100 : 8073\n"
     ]
    }
   ],
   "source": [
    "array_ATS_0_50_fp = '/Volumes/Extreme SSD/CAPSTONE_DATA/saved/ATS_0_50'\n",
    "array_ATS_51_100_fp = '/Volumes/Extreme SSD/CAPSTONE_DATA/saved/ATS_51_100'\n",
    "\n",
    "arrays_ATS_0_50 = os.listdir(array_ATS_0_50_fp)\n",
    "arrays_ATS_51_100 = os.listdir(array_ATS_51_100_fp)\n",
    "print(\"No. of arrays in folders 000 - 050 :\", len(arrays_ATS_0_50))\n",
    "print(\"No. of arrays in folders 051 - 100 :\", len(arrays_ATS_51_100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction options\n",
    "1. Save all feature arrays for a given track_id in a dictionary (~16,000 dictionarys {1 track : 25 features})\n",
    "2. Save all tracks' feature arrays in a dictionary  (~25 dictionarys {16,000 tracks : same feature})\n",
    "\n",
    "### Classification methods\n",
    "\n",
    "**A.** Descriptive stats on each feature ---> model entirely on this ---> predictions...etc.\n",
    "\n",
    "**B.** PCA each feature (spectrogram) ---> model on each one individually ---> majority vote predictions...etc.\n",
    "\n",
    "**B.** PCA black & white, spectrogram ---> model on this entirely ---> predictions...etc.\n",
    "\n",
    "**C.** PCA each feature ---> model entirely on this ---> predictions...etc.\n",
    "\n",
    "***Which extraction option benefits which classification method?***\n",
    "\n",
    "Method A: 1\n",
    "\n",
    "Method B : 2\n",
    "\n",
    "Method C: 1 or 2 ?\n",
    "\n",
    "Option 1 will be better to iterate over and save after each track has its 25 features extracted, otherwise could lose.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import pickle\n",
    "\n",
    "def extract_features_array(y, track_id, save = True, display = False):  #takes an ATS array and track_id to give all features for given ATS in dictionary\n",
    "\n",
    "    #Spectral \n",
    "    S1 = np.abs(librosa.stft(y=y, n_fft=2048))**2  #dont save\n",
    "    S2 = np.abs(librosa.stft(y=y))  #dont save             \n",
    "    yharm = librosa.effects.harmonic(y)            \n",
    "    \n",
    "    melspec = librosa.feature.melspectrogram(y = y, hop_length=512)\n",
    "    #chroma_stft_y = librosa.feature.chroma_stft(y = y, n_chroma=12, hop_length=512) #dont save\n",
    "    chroma_stft_S1 = librosa.feature.chroma_stft(S = S1, n_chroma=12) \n",
    "    chroma_cens = librosa.feature.chroma_cens(y = y, n_chroma=12)   \n",
    "    mfcc = librosa.feature.mfcc(y = y, n_mfcc = 12)\n",
    "    rms = librosa.feature.rms(y = y, S = melspec)    \n",
    "    spec_centroid = librosa.feature.spectral_centroid(y = y)\n",
    "    \n",
    "    spec_bw = librosa.feature.spectral_bandwidth(y=y)\n",
    "    contrast = librosa.feature.spectral_contrast(S = S1)\n",
    "    flatness = librosa.feature.spectral_flatness(y=y)\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y) \n",
    "    poly = librosa.feature.poly_features(S = S1, order=0)\n",
    "    tonnetz = librosa.feature.tonnetz(y= yharm)\n",
    "    ZCR = librosa.feature.zero_crossing_rate(y = y)\n",
    "    \n",
    "    #Rhythm\n",
    "    oenv = librosa.onset.onset_strength(y=y, hop_length=512) #dont save\n",
    "    \n",
    "    tempo = librosa.beat.tempo(onset_envelope = oenv, hop_length=512)[0]\n",
    "    \n",
    "    #Spectogram Decomposition\n",
    "    S3 = librosa.stft(y=y, hop_length=512)\n",
    "    \n",
    "    H, P = librosa.decompose.hpss(S = S3)  \n",
    "    \n",
    "    items = [('track_id', track_id), ('yharm', yharm),('melspec', melspec),('chroma_stft_S1', chroma_stft_S1), \n",
    "             ('chroma_cens', chroma_cens),('mfcc', mfcc), ('rms', rms), ('spec_centroid', spec_centroid), ('spec_bw', spec_bw),\n",
    "            ('contrast', contrast),('flatness', flatness),('rolloff', rolloff),('poly', poly),('tonnetz', tonnetz),\n",
    "            ('ZCR', ZCR),('tempo', tempo),('H', H),('P', P)]\n",
    "    \n",
    "    dictionary = {}\n",
    "    \n",
    "    for key, value in items:       #filling dictionary with items list\n",
    "        dictionary[key] = value\n",
    "        \n",
    "    if save:\n",
    "        feat_dict = open('/Volumes/Extreme SSD/CAPSTONE_DATA/saved/features_dictionaries/dict_feat_{}.pkl'.format(track_id), \"wb\")\n",
    "        pickle.dump(dictionary, feat_dict)\n",
    "        feat_dict.close()       \n",
    "    \n",
    "    if display:\n",
    "        return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test above function, displaying feature array extracted for a given track using the ATS numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'track_id': '050993',\n",
       " 'yharm': array([-0.07391745, -0.08061884, -0.08036947, ...,  0.        ,\n",
       "         0.        ,  0.        ], dtype=float32),\n",
       " 'melspec': array([[3.8379120e+01, 1.3121475e+01, 1.6021961e+01, ..., 1.9974895e-01,\n",
       "         4.9430913e-01, 1.3632834e-01],\n",
       "        [6.3905707e+02, 6.2784198e+02, 6.4522736e+02, ..., 5.5490702e-01,\n",
       "         1.0584415e+00, 3.8813183e-01],\n",
       "        [1.9068240e+02, 2.5939569e+02, 3.2135941e+02, ..., 3.5227637e+00,\n",
       "         3.8852777e+00, 1.2624512e+00],\n",
       "        ...,\n",
       "        [1.5188937e-02, 1.3670291e-02, 1.3535739e-02, ..., 6.7426945e-04,\n",
       "         5.9079559e-04, 2.7281829e-04],\n",
       "        [1.0077956e-02, 5.4014931e-03, 3.2646512e-03, ..., 1.7821505e-04,\n",
       "         1.0034796e-04, 3.0683008e-05],\n",
       "        [3.0446649e-04, 1.4727877e-04, 6.6611567e-05, ..., 1.3548350e-05,\n",
       "         8.7201297e-06, 5.9221502e-06]], dtype=float32),\n",
       " 'chroma_stft_S1': array([[1.        , 0.9077362 , 0.7660032 , ..., 0.9297181 , 1.        ,\n",
       "         1.        ],\n",
       "        [0.61988   , 0.5038569 , 0.48371005, ..., 0.34176502, 0.43148735,\n",
       "         0.47745723],\n",
       "        [0.29518464, 0.22572471, 0.25370154, ..., 0.06967106, 0.14103927,\n",
       "         0.19940923],\n",
       "        ...,\n",
       "        [0.5794369 , 0.5643983 , 0.567753  , ..., 0.25159347, 0.23961014,\n",
       "         0.53216803],\n",
       "        [0.74263287, 0.8658278 , 0.9452543 , ..., 0.7930275 , 0.7578575 ,\n",
       "         0.83031493],\n",
       "        [0.9081314 , 1.        , 1.        , ..., 1.        , 0.99598354,\n",
       "         0.96771985]], dtype=float32),\n",
       " 'chroma_cens': array([[0.32436939, 0.33366595, 0.34309042, ..., 0.55151476, 0.54320607,\n",
       "         0.53463741],\n",
       "        [0.23357785, 0.23975198, 0.24665793, ..., 0.33192746, 0.32307078,\n",
       "         0.31446519],\n",
       "        [0.03837095, 0.04852034, 0.05981118, ..., 0.14850491, 0.14020756,\n",
       "         0.13213736],\n",
       "        ...,\n",
       "        [0.2189309 , 0.22039837, 0.22198233, ..., 0.19863743, 0.19585821,\n",
       "         0.19265357],\n",
       "        [0.56140026, 0.54862063, 0.53480218, ..., 0.38094434, 0.3872636 ,\n",
       "         0.39302167],\n",
       "        [0.65352478, 0.65572542, 0.65763179, ..., 0.58910324, 0.60045202,\n",
       "         0.6117965 ]]),\n",
       " 'mfcc': array([[-109.717125 , -122.796684 , -150.93538  , ..., -224.42401  ,\n",
       "         -236.83347  , -275.01974  ],\n",
       "        [  64.14166  ,   68.06891  ,   68.741936 , ...,   84.0254   ,\n",
       "           78.807816 ,   71.9601   ],\n",
       "        [  21.360905 ,   23.740498 ,   41.595867 , ...,   26.762949 ,\n",
       "           24.42715  ,   16.011742 ],\n",
       "        ...,\n",
       "        [  13.903521 ,   11.098112 ,    9.65193  , ...,    6.7956176,\n",
       "            7.824309 ,    4.298038 ],\n",
       "        [  -9.76483  ,   -9.972653 ,  -11.609228 , ...,   -2.0642705,\n",
       "            2.439507 ,    8.444502 ],\n",
       "        [   4.365325 ,    7.357996 ,   13.424087 , ...,    5.574071 ,\n",
       "            7.7497873,    9.856885 ]], dtype=float32),\n",
       " 'rms': array([[0.20283848, 0.19679226, 0.18631095, ..., 0.13286893, 0.10087709,\n",
       "         0.065805  ]], dtype=float32),\n",
       " 'spec_centroid': array([[2209.68951142, 2129.81364539, 1815.71510979, ..., 1451.43176838,\n",
       "         1744.29955564, 2242.03899551]]),\n",
       " 'spec_bw': array([[2962.48528551, 2904.75364563, 2785.35698834, ..., 2445.65987156,\n",
       "         2620.88530086, 2803.05176418]]),\n",
       " 'contrast': array([[48.26588027, 40.87312494, 43.33066613, ..., 38.87196464,\n",
       "         29.59485922, 36.06576041],\n",
       "        [35.43938221, 42.88051926, 33.0006457 , ..., 27.31108672,\n",
       "         21.20479962, 25.99663719],\n",
       "        [40.5412448 , 31.89422427, 28.90906956, ..., 28.38845589,\n",
       "         21.2421222 , 22.06508874],\n",
       "        ...,\n",
       "        [41.35238072, 39.40500396, 42.15516814, ..., 32.45401875,\n",
       "         26.26393629, 33.75203658],\n",
       "        [46.49445345, 35.96769513, 39.14305849, ..., 34.94554171,\n",
       "         35.86855954, 32.26839185],\n",
       "        [50.13421323, 43.5430506 , 67.81440437, ..., 53.24137733,\n",
       "         51.05074934, 46.89130855]]),\n",
       " 'flatness': array([[0.00438299, 0.0041235 , 0.00125578, ..., 0.00078376, 0.00233394,\n",
       "         0.00869203]], dtype=float32),\n",
       " 'rolloff': array([[6040.06347656, 5975.46386719, 5351.00097656, ..., 3757.54394531,\n",
       "         4554.27246094, 5404.83398438]]),\n",
       " 'poly': array([[31.28540396, 30.23587676, 32.00914315, ..., 15.43503011,\n",
       "          4.93624232,  0.7030848 ]]),\n",
       " 'tonnetz': array([[-0.22127074, -0.22069294, -0.22265718, ...,  0.1034167 ,\n",
       "          0.08463761,  0.07036311],\n",
       "        [-0.03207529, -0.02588714, -0.02173809, ..., -0.23997729,\n",
       "         -0.19103508, -0.15359297],\n",
       "        [ 0.12163498,  0.1182733 ,  0.11038714, ...,  0.36548419,\n",
       "          0.34196751,  0.32363081],\n",
       "        [-0.2766744 , -0.27885562, -0.27729448, ..., -0.03425618,\n",
       "         -0.02869485, -0.02780772],\n",
       "        [ 0.06191218,  0.0616849 ,  0.06121208, ..., -0.13256778,\n",
       "         -0.11589181, -0.10147678],\n",
       "        [-0.11948343, -0.11972259, -0.11896693, ..., -0.07714311,\n",
       "         -0.0682922 , -0.06178791]]),\n",
       " 'ZCR': array([[0.02880859, 0.04199219, 0.05224609, ..., 0.01855469, 0.04150391,\n",
       "         0.03857422]]),\n",
       " 'tempo': 135.99917763157896,\n",
       " 'H': array([[-8.66834307e-04+1.30888342e-10j,  4.32477659e-03+0.00000000e+00j,\n",
       "          5.21521689e-03+0.00000000e+00j, ...,\n",
       "          9.48648691e-01+0.00000000e+00j,  1.69469106e+00+0.00000000e+00j,\n",
       "          1.61868680e+00+0.00000000e+00j],\n",
       "        [ 3.29377912e-02-5.96982710e-19j, -1.09762244e-01+2.77923550e-02j,\n",
       "          2.16364324e-01-4.85705808e-02j, ...,\n",
       "         -1.67488545e-01+4.36490864e-01j, -1.40842140e+00+8.45163167e-02j,\n",
       "         -3.41252834e-01-9.99201238e-01j],\n",
       "        [-4.91995364e-01-7.42892325e-08j,  2.00520530e-01+1.25785798e-01j,\n",
       "         -9.90506709e-02+5.42405486e-01j, ...,\n",
       "         -4.37059671e-01-1.11025505e-01j,  7.85585165e-01-1.28659001e-02j,\n",
       "         -4.54636186e-01+5.92688501e-01j],\n",
       "        ...,\n",
       "        [ 1.11319059e-06+1.49506353e-20j, -2.20048241e-06+7.73507125e-09j,\n",
       "         -1.70422063e-05-2.44721991e-06j, ...,\n",
       "          1.75633850e-05-5.66990639e-05j,  6.67849672e-05-1.73935064e-06j,\n",
       "         -1.46044840e-05+7.64057768e-06j],\n",
       "        [-6.55127849e-07+9.89215559e-14j,  6.92801594e-10+1.30170611e-06j,\n",
       "         -8.81296558e-07+2.73295768e-06j, ...,\n",
       "         -2.42197912e-05-3.20843828e-05j, -1.88088361e-05+1.07860587e-05j,\n",
       "          4.82151927e-06+4.63404058e-06j],\n",
       "        [ 3.67373673e-07+0.00000000e+00j,  7.29850569e-07+0.00000000e+00j,\n",
       "         -1.51055735e-06+2.28087812e-13j, ...,\n",
       "          3.07499022e-06+0.00000000e+00j,  1.24162952e-05+0.00000000e+00j,\n",
       "         -2.67321047e-06+4.03643563e-13j]], dtype=complex64),\n",
       " 'P': array([[-4.41583842e-01+6.6677309e-08j,  1.00133228e+00+0.0000000e+00j,\n",
       "          7.39133716e-01+0.0000000e+00j, ...,\n",
       "          1.69188929e+00+0.0000000e+00j,  2.37379265e+00+0.0000000e+00j,\n",
       "          1.04273522e+00+0.0000000e+00j],\n",
       "        [ 1.77361202e+00-3.2145923e-17j, -2.68630075e+00+6.8018496e-01j,\n",
       "          2.43215775e+00-5.4598331e-01j, ...,\n",
       "         -3.32088560e-01+8.6545396e-01j, -2.00106692e+00+1.2007969e-01j,\n",
       "         -2.27085710e-01-6.6491556e-01j],\n",
       "        [-9.70380974e+00-1.4652345e-06j,  3.73228884e+00+2.3412514e+00j,\n",
       "         -8.46795142e-01+4.6370845e+00j, ...,\n",
       "         -9.97452378e-01-2.5338107e-01j,  1.40808988e+00-2.3060953e-02j,\n",
       "         -3.48225176e-01+4.5396528e-01j],\n",
       "        ...,\n",
       "        [ 4.06043828e-02+5.4533458e-16j, -2.02722680e-02+7.1260474e-05j,\n",
       "         -3.83927581e-05-5.5131081e-06j, ...,\n",
       "          2.50999892e-05-8.1029131e-05j,  8.95679332e-05-2.3327113e-06j,\n",
       "         -7.27926745e-05+3.8082690e-05j],\n",
       "        [-4.05140333e-02+6.1174488e-09j,  1.07797550e-05+2.0254100e-02j,\n",
       "         -4.95538006e-06+1.5366955e-05j, ...,\n",
       "         -5.56954365e-05-7.3780720e-05j, -7.02306133e-05+4.0274241e-05j,\n",
       "          6.69076835e-05+6.4306063e-05j],\n",
       "        [ 4.05075066e-02+0.0000000e+00j,  2.02479959e-02+0.0000000e+00j,\n",
       "         -2.92580553e-05+4.4178437e-12j, ...,\n",
       "          1.13977785e-05+0.0000000e+00j,  5.91281751e-05+0.0000000e+00j,\n",
       "         -4.73111068e-05+7.1437786e-12j]], dtype=complex64)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test above function\n",
    "a_array = open('/Volumes/Extreme SSD/CAPSTONE_DATA/saved/ATS_0_50/050993.npy', \"rb\")\n",
    "ATS_TEST = np.load(a_array)\n",
    "\n",
    "extract_features_array(ATS_TEST, '050993', save = False, display=True) #not saving only displaying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running above function, each stored dictionary created takes ~ 25 MB. Estimate total 16,000 tracks required space : 400 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***EXTRACTING*** \n",
    "\n",
    "Getting feature dictionaries for tracks in folders 0 - 50 and saving to hard drive (*Do not run twice*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/librosa/core/pitch.py:153: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  warnings.warn(\"Trying to estimate tuning from empty frequency set.\")\n"
     ]
    }
   ],
   "source": [
    "ATS_0_50_fp = '/Volumes/Extreme SSD/CAPSTONE_DATA/saved/ATS_0_50'\n",
    "ATS_0_50 = os.listdir(ATS_0_50_fp)\n",
    "\n",
    "for ATS in ATS_0_50:\n",
    "    track_id = ATS.split('.')[0]\n",
    "    a_array = open('/Volumes/Extreme SSD/CAPSTONE_DATA/saved/ATS_0_50/{}'.format(ATS), \"rb\")\n",
    "    y = np.load(a_array)\n",
    "    extract_features_array(y, track_id, save = True, display = False) #automatically saves each dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking total No. of feature dictionaries matches total No. of tracks and ATS numpy arrays (7,949)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of feature dictionaries in folders 000 - 050 : 7949\n"
     ]
    }
   ],
   "source": [
    "feat_dict_0_50_fp = '/Volumes/Extreme SSD/CAPSTONE_DATA/saved/features_dictionaries' #numpy arrays that were extracted\n",
    "feat_dict_0_50 = os.listdir(feat_dict_0_50_fp)\n",
    "\n",
    "print(\"No. of feature dictionaries in folders 000 - 050 :\", len(feat_dict_0_50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting feature dictionaries for tracks in folders 51 - 100 and saving to hard drive (*Do not run twice*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/librosa/core/pitch.py:153: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  warnings.warn(\"Trying to estimate tuning from empty frequency set.\")\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=1024 is too small for input signal of length=558\n",
      "  n_fft, y.shape[-1]\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=1024 is too small for input signal of length=720\n",
      "  n_fft, y.shape[-1]\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=1024 is too small for input signal of length=360\n",
      "  n_fft, y.shape[-1]\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=1024 is too small for input signal of length=180\n",
      "  n_fft, y.shape[-1]\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=1024 is too small for input signal of length=531\n",
      "  n_fft, y.shape[-1]\n"
     ]
    }
   ],
   "source": [
    "ATS_51_100_fp = '/Volumes/Extreme SSD/CAPSTONE_DATA/saved/ATS_51_100'\n",
    "ATS_51_100 = os.listdir(ATS_51_100_fp)\n",
    "\n",
    "for ATS in ATS_51_100:\n",
    "    track_id = ATS.split('.')[0]\n",
    "    a_array = open('/Volumes/Extreme SSD/CAPSTONE_DATA/saved/ATS_51_100/{}'.format(ATS), \"rb\")\n",
    "    y = np.load(a_array)\n",
    "    extract_features_array(y, track_id, save = True, display = False) #automatically saves each dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting feature dictionaries from folders 51 - 100 verify that total in folder 'feature_dictionaries' increased to 7949 + 8073 = 16022 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of feature dictionaries in folders 000 - 100 : 16022\n"
     ]
    }
   ],
   "source": [
    "feat_dict_0_100_fp = '/Volumes/Extreme SSD/CAPSTONE_DATA/saved/features_dictionaries' #numpy arrays that were extracted\n",
    "feat_dict_0_100 = os.listdir(feat_dict_0_100_fp)\n",
    "\n",
    "print(\"No. of feature dictionaries in folders 000 - 100 :\", len(feat_dict_0_100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
