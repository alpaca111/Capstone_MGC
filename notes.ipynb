{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display   #for loading and visualizing audio files\n",
    "import IPython.display as ipd   #to play audio\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "import time \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start writing up presentation as you go after modelling stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 methods:\n",
    "\n",
    "**A.** PCA each feature (spectrogram) ---> save each PCA'd feature into own dataframe ---> model on each one individually --> majority vote predictions\n",
    "\n",
    "**B.** From PCA'd dataframe combine into one big and model entirely ---> predictions\n",
    "\n",
    "**C.** Do descriptive stats and model on this ---> predictions\n",
    "\n",
    "\n",
    "- simple model on various features count predictions and choose from majority vote\n",
    "\n",
    "(chroma, tempo, spectogram, mse)\n",
    "- MAKE SURE each AUDIO file is same sample rate!\n",
    "\n",
    "\n",
    "- turn into black & white images, pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N. of frames from hop length and y**\n",
    "\n",
    "If you divide your y_shape length by the hop_length - you get the number of the frames. \n",
    "\n",
    "- y.shape/hop_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regarding warning message :**\n",
    "\n",
    "There is nothing to fix: this is operating as intended.\n",
    "\n",
    "Again, this is not an error, it is a warning. Your code will work just fine, it just will fall back on the (slower) audioread decoder. This is unavoidable for now, since libsndfile still does not support mp3.\n",
    "\n",
    "If you're really concerned about speed, you could transcode your mp3s into a more permissive format (eg ogg vorbis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "chroma_stft([y, sr, S, norm, n_fft, …])            Compute a chromagram from a waveform or power spectrogram.\n",
    "\n",
    "chroma_cqt([y, sr, C, hop_length, fmin, …])        Constant-Q chromagram\n",
    "\n",
    "chroma_cens([y, sr, C, hop_length, fmin, …])       Computes the chroma variant “Chroma Energy Normalized” (CENS)\n",
    "\n",
    "melspectrogram([y, sr, S, n_fft, …])               Compute a mel-scaled spectrogram.\n",
    "\n",
    "mfcc([y, sr, S, n_mfcc, dct_type, norm, lifter])   Mel-frequency cepstral coefficients (MFCCs)\n",
    "\n",
    "rms([y, S, frame_length, hop_length, …])           Compute root-mean-square (RMS) value for each frame, either from the audio samples y or from a spectrogram S.\n",
    "\n",
    "spectral_centroid([y, sr, S, n_fft, …])            Compute the spectral centroid.\n",
    "\n",
    "spectral_bandwidth([y, sr, S, n_fft, …])           Compute p’th-order spectral bandwidth.\n",
    "\n",
    "spectral_contrast([y, sr, S, n_fft, …])            Compute spectral contrast\n",
    "\n",
    "spectral_flatness([y, S, n_fft, hop_length, …])    Compute spectral flatness\n",
    "\n",
    "spectral_rolloff([y, sr, S, n_fft, …])             Compute roll-off frequency.\n",
    "\n",
    "poly_features([y, sr, S, n_fft, hop_length, …])    Get coefficients of fitting an nth-order polynomial to the columns of a spectrogram.\n",
    "\n",
    "tonnetz([y, sr, chroma])                           Computes the tonal centroid features (tonnetz)\n",
    "\n",
    "zero_crossing_rate(y[, frame_length, …])           Compute the zero-crossing rate of an audio time series.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Yeah pretty much - Librosa turns the raw audio into a 1D numpy array with X numbers per second \n",
    "(you define the sample rate)\n",
    "[9:29 AM] You can then use this array to compute all the other features, which take samples over this array\n",
    "[9:30 AM] So at this point your music is a time series but you have 1 series per different feature\n",
    "[9:30 AM] I just took the mean and standard deviation from all these features.\n",
    "[9:31 AM] I think that’s part of the reason people use images. It’s a better way of capturing all the information \n",
    "and convolutional neural networks use a 2D filter rather than using SKLearn which would have to use a 1d numpy array\n",
    "[9:31 AM] It someone else who wrote the paper, you can check it out here: \n",
    "https://qmro.qmul.ac.uk/xmlui/bitstream/handle/123456789/46029/CHOI_Keunwoo_PhD_Final_190918.pdf?sequence=1&isAllowed=y\n",
    "[9:31 AM] Is someone planning on doing a music project?\n",
    "\n",
    "Finn Alexander  [9:37 AM]\n",
    "Congratulations, thats very exciting about CapGemini!\n",
    "[9:39 AM] yeah two people are planning on doing music classification, seems to be all the rage!\n",
    "[9:42 AM] Just so i understand you took the mean and std across the time series for each feature or frequency or \n",
    "did you take it across the frequencies? Did you try classifying using chroma frequencies as well as spectrograms. \n",
    "(edited) \n",
    "[9:42 AM] thanks for the link\n",
    "\n",
    "Adam Shafi  [9:59 AM]\n",
    "Thanks! I’m really excited!\n",
    "[9:59 AM] Wow it is all the rage! Are they both using GTZAN?\n",
    "[9:59 AM] Yes I took the mean and std across the time series for each feature\n",
    "[9:59 AM] I used chroma, mfcc, tempo and many others\n",
    "[10:00 AM] but even using all of those, using black and white images with dimensionality reduction and \n",
    "sklearn models performed way better\n",
    "\n",
    "Finn Alexander  [11:10 AM]\n",
    "Very interesting, I imagine you tried all of this convolutional networks stuff. Will have a look a t GTZAN and push \n",
    "them in that direction? (edited) \n",
    "\n",
    "Adam Shafi  [11:33 AM]\n",
    "GTZAN is a dataset, Joh used it, its really popular and probably a good choice.\n",
    "Using convolutional neural networks is pretty hard and on GTZAN you can get some good results without them. \n",
    "Take a look on Kaggle and there are some great notebooks!\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old way to collect audio time series and create dataframe \n",
    "\n",
    "As it is lower to retrieve from pandas dataframe, decided to use pickle dump to store numpy array files on harddrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all filepaths for a given folder and store the audio time series y, sr = librosa.load(test_filepath, duration =25)\n",
    "\n",
    "fp_main = '/Volumes/Extreme SSD/CAPSTONE_DATA/fma_medium/'\n",
    "folders = os.listdir(fp_main)\n",
    "\n",
    "numbers = ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "track_id_list = []\n",
    "audio_timeseries_list = []  #saved as a tuple (y,sr)\n",
    "filepath_list = []\n",
    "folder_list = []\n",
    "error_list = []\n",
    "\n",
    "#tranch this with finns code\n",
    "for folder_numb in folders:\n",
    "    if folder_numb == '101': # limit till what file number you want to collect. \n",
    "        break\n",
    "    if any(x in folder_numb for x in numbers):  #to prevent picking up hidden files such as 'README.txt' or 'checksums'\n",
    "        print(\"folder :\",folder_numb)\n",
    "        fp_main_new = fp_main + folder_numb + '/'  #not forget the '/' at the end\n",
    "        audio_clips = os.listdir(fp_main_new)\n",
    "        for track_id in audio_clips:\n",
    "            if '_' in track_id:\n",
    "                pass\n",
    "            else:\n",
    "                fp_new = fp_main_new + track_id    \n",
    "                try:                #need try and except as y,sr extraction doesnt work for all e.g '001486.mp3'\n",
    "                    #fp_new = fp_main_new + track_id              #fp for filepath\n",
    "                    y,sr = librosa.load(fp_new, duration = 30) #clips are 30secs\n",
    "    \n",
    "                    track_id_list.append(track_id.split('.')[0])\n",
    "                    audio_timeseries_list.append((y,sr)) \n",
    "                    error_list.append('none') \n",
    "                    filepath_list.append(fp_new) \n",
    "                    folder_list.append(folder_numb)\n",
    "                except:\n",
    "                    track_id_list.append(track_id.split('.')[0]) #track_id\n",
    "                    audio_timeseries_list.append('error') #numpy array\n",
    "                    error_list.append('error')\n",
    "                    filepath_list.append(fp_new) \n",
    "                    folder_list.append(folder_numb)\n",
    "    else:\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_series = pd.DataFrame({'track_id': track_id_list,'time_series' : audio_timeseries_list, \n",
    "                               'error_track' : error_list,'folder' : folder_list,'filepath' : filepath_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dict_feat_000139.pkl'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#alternative Getting summary statistic feature dictionaries for all numpy arrays stored in feature_dictionaries*** (*Do not run twice*\n",
    "for feat_dict in feature_dict_0_50:\n",
    "    track_id = feature_dict_0_50[i].split('_')[2].split('.')[0]\n",
    "    \n",
    "    load_feat_from = open('/Volumes/Extreme SSD/CAPSTONE_DATA/saved/features_dictionaries/{}'.format(feat_dict), \"rb\")\n",
    "    a_feat_dict = pickle.load(load_feat_from)\n",
    "    \n",
    "    get_statistics_dict(a_feat_dict, reduce_limit = 50)\n",
    "    \n",
    "    save_ss_feat_dict_to = open('/Volumes/Extreme SSD/CAPSTONE_DATA/saved/ss_feature_dictionaries/dict_ss_feat_{}.pkl'.format(track_id), \"wb\")\n",
    "    pickle.dump(dictionary, save_ss_feat_dict_to)\n",
    "    save_ss_feat_dict_to.close()   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>time_series</th>\n",
       "      <th>error_track</th>\n",
       "      <th>folder</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000002</td>\n",
       "      <td>([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>None</td>\n",
       "      <td>000</td>\n",
       "      <td>/Volumes/Extreme SSD/CAPSTONE_DATA/fma_medium/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000003</td>\n",
       "      <td>([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>None</td>\n",
       "      <td>000</td>\n",
       "      <td>/Volumes/Extreme SSD/CAPSTONE_DATA/fma_medium/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000005</td>\n",
       "      <td>([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>None</td>\n",
       "      <td>000</td>\n",
       "      <td>/Volumes/Extreme SSD/CAPSTONE_DATA/fma_medium/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000010</td>\n",
       "      <td>([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>None</td>\n",
       "      <td>000</td>\n",
       "      <td>/Volumes/Extreme SSD/CAPSTONE_DATA/fma_medium/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000134</td>\n",
       "      <td>([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>None</td>\n",
       "      <td>000</td>\n",
       "      <td>/Volumes/Extreme SSD/CAPSTONE_DATA/fma_medium/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>001965</td>\n",
       "      <td>([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>None</td>\n",
       "      <td>001</td>\n",
       "      <td>/Volumes/Extreme SSD/CAPSTONE_DATA/fma_medium/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>001966</td>\n",
       "      <td>([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>None</td>\n",
       "      <td>001</td>\n",
       "      <td>/Volumes/Extreme SSD/CAPSTONE_DATA/fma_medium/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>001967</td>\n",
       "      <td>([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>None</td>\n",
       "      <td>001</td>\n",
       "      <td>/Volumes/Extreme SSD/CAPSTONE_DATA/fma_medium/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>001995</td>\n",
       "      <td>([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>None</td>\n",
       "      <td>001</td>\n",
       "      <td>/Volumes/Extreme SSD/CAPSTONE_DATA/fma_medium/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>001996</td>\n",
       "      <td>([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>None</td>\n",
       "      <td>001</td>\n",
       "      <td>/Volumes/Extreme SSD/CAPSTONE_DATA/fma_medium/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>415 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    track_id                                        time_series error_track  \\\n",
       "0     000002  ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...        None   \n",
       "1     000003  ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...        None   \n",
       "2     000005  ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...        None   \n",
       "3     000010  ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...        None   \n",
       "4     000134  ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...        None   \n",
       "..       ...                                                ...         ...   \n",
       "410   001965  ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...        None   \n",
       "411   001966  ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...        None   \n",
       "412   001967  ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...        None   \n",
       "413   001995  ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...        None   \n",
       "414   001996  ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...        None   \n",
       "\n",
       "    folder                                           filepath  \n",
       "0      000  /Volumes/Extreme SSD/CAPSTONE_DATA/fma_medium/...  \n",
       "1      000  /Volumes/Extreme SSD/CAPSTONE_DATA/fma_medium/...  \n",
       "2      000  /Volumes/Extreme SSD/CAPSTONE_DATA/fma_medium/...  \n",
       "3      000  /Volumes/Extreme SSD/CAPSTONE_DATA/fma_medium/...  \n",
       "4      000  /Volumes/Extreme SSD/CAPSTONE_DATA/fma_medium/...  \n",
       "..     ...                                                ...  \n",
       "410    001  /Volumes/Extreme SSD/CAPSTONE_DATA/fma_medium/...  \n",
       "411    001  /Volumes/Extreme SSD/CAPSTONE_DATA/fma_medium/...  \n",
       "412    001  /Volumes/Extreme SSD/CAPSTONE_DATA/fma_medium/...  \n",
       "413    001  /Volumes/Extreme SSD/CAPSTONE_DATA/fma_medium/...  \n",
       "414    001  /Volumes/Extreme SSD/CAPSTONE_DATA/fma_medium/...  \n",
       "\n",
       "[415 rows x 5 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time_series"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
